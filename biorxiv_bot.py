# biorxiv_bot.py
"""
BioRxiv è‚¿ç˜¤å­¦æ–‡ç« è‡ªåŠ¨é˜…è¯»ä¸é‚®ä»¶æ¨é€ç³»ç»Ÿ
- å®šæ—¶è·å– BioRxiv æœ€æ–°æ–‡ç« 
- ä½¿ç”¨ AI ç”Ÿæˆç ”ç©¶ç®€æŠ¥
- é€šè¿‡é‚®ä»¶å‘é€ç»™ç”¨æˆ·
"""

import os
import json
import yaml
import asyncio
import time
from datetime import datetime
from dotenv import load_dotenv
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from dateutil import tz
import logging

from utils import now_in_tz, last_window_start, fmt_period
from biorxiv_fetch import fetch_window, pack_papers
from pubmed_fetch import fetch_pubmed_articles
from summarizer_api import run_ollama
from state import PeriodState
from email_sender import send_digest_email, send_error_notification

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('biorxiv_push.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åŠ è½½é…ç½®
try:
    with open("config.yaml", "r", encoding="utf-8") as f:
        CFG = yaml.safe_load(f)
    logger.info("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
except FileNotFoundError:
    logger.error("âŒ æœªæ‰¾åˆ° config.yaml æ–‡ä»¶ï¼Œè¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶")
    exit(1)

TZNAME = CFG.get("timezone", "Asia/Shanghai")
WINDOW_H = int(CFG.get("time_window_hours", 24))

# å…¨å±€çŠ¶æ€
BOT_STATUS = {
    "running": True,
    "start_time": datetime.now(),
    "last_fetch": None,
    "last_report": None,
    "total_reports": 0,
    "total_papers": 0,
    "errors": []
}

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = AsyncIOScheduler(timezone=TZNAME)


async def generate_and_send_digest(period_label: str, manual=False):
    """
    ç”Ÿæˆå¹¶å‘é€ç ”ç©¶ç®€æŠ¥
    
    Args:
        period_label: æœŸåˆ«æ ‡ç­¾ï¼ˆå¦‚ï¼šæ—©æŠ¥ã€æ™šæŠ¥ï¼‰
        manual: æ˜¯å¦ä¸ºæ‰‹åŠ¨è§¦å‘
    """
    try:
        logger.info("=" * 80)
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆ{period_label}")
        logger.info("=" * 80)
        
        # è®¡ç®—æ—¶é—´çª—å£
        now_local = now_in_tz(TZNAME)
        since_local = last_window_start(TZNAME, WINDOW_H)
        
        logger.info(f"ğŸ“… æ—¶é—´çª—å£: {since_local} ~ {now_local}")
        
        # è·å–æ–‡ç« 
        logger.info("ğŸ“¥ æ­£åœ¨ä» BioRxiv è·å–æ–‡ç« ...")
        biorxiv_articles = fetch_window(CFG, since_local, now_local)
        biorxiv_data = pack_papers(CFG, biorxiv_articles)
        
        logger.info(f"âœ… BioRxiv: {len(biorxiv_data)} ç¯‡æ–‡ç« ")
        
        # è·å– PubMed æ–‡ç« ï¼ˆNature/Science/Cellç­‰ï¼‰
        logger.info("ğŸ“¥ æ­£åœ¨ä» PubMed è·å–é¡¶çº§æœŸåˆŠæ–‡ç« ...")
        try:
            pubmed_articles = fetch_pubmed_articles(CFG, days=3)
            pubmed_data = pack_papers(CFG, pubmed_articles)
            logger.info(f"âœ… PubMed: {len(pubmed_data)} ç¯‡æ–‡ç« ")
        except Exception as e:
            logger.warning(f"âš ï¸ PubMed è·å–å¤±è´¥: {e}")
            pubmed_data = []
        
        # åˆå¹¶æ•°æ®
        data = biorxiv_data + pubmed_data
        logger.info(f"ğŸ“Š åˆå¹¶åæ€»è®¡: {len(data)} ç¯‡æ–‡ç« ")
        
        BOT_STATUS["last_fetch"] = now_local
        BOT_STATUS["total_papers"] += len(data)
        
        logger.info(f"âœ… è·å–åˆ° {len(data)} ç¯‡æ–‡ç« ")
        
        # å¦‚æœæ²¡æœ‰æ–‡ç« 
        if len(data) == 0:
            logger.warning("âš ï¸ æœ¬æ¬¡æ—¶é—´çª—å£å†…æ²¡æœ‰æ–°æ–‡ç« ")
            # å¯é€‰ï¼šå‘é€æ— æ–‡ç« é€šçŸ¥é‚®ä»¶
            # send_digest_email(period_label, "# æœ¬æ¬¡æ—¶é—´çª—å£å†…æ²¡æœ‰æ–°çš„è‚¿ç˜¤å­¦ç›¸å…³æ–‡ç« \n\nè¯·ç¨åå†è¯•ã€‚")
            return True
        
        # ä¿å­˜åŸå§‹æ•°æ®
        period = fmt_period(now_local)
        st = PeriodState(period)
        st.save_raw(data)
        logger.info(f"ğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜: {period}")
        
        # ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦
        logger.info("ğŸ¤– æ­£åœ¨ä½¿ç”¨ AI ç”Ÿæˆç ”ç©¶ç®€æŠ¥...")
        logger.info(f"   ä½¿ç”¨æ¨¡å‹: DeepSeek-V3.2-Exp")
        
        summary_md = run_ollama(
            CFG,
            period_label,
            since_local.isoformat(),
            now_local.isoformat(),
            json.dumps(data, ensure_ascii=False)
        )
        
        st.save_report(summary_md)
        logger.info(f"ğŸ’¾ ç®€æŠ¥å·²ä¿å­˜")
        
        # ç”Ÿæˆä¸Šä¸‹æ–‡ï¼ˆç”¨äºåç»­å¯¹è¯ï¼‰
        prompt_ctx = (
            "# åŸå§‹æ¡ç›® (JSON)\n" +
            json.dumps(data, ensure_ascii=False, indent=2) +
            "\n\n# ç ”ç©¶ç®€æŠ¥ (Markdown)\n" +
            summary_md
        )
        st.save_prompt(prompt_ctx)
        
        # å‘é€é‚®ä»¶
        logger.info("ğŸ“§ æ­£åœ¨å‘é€é‚®ä»¶...")
        email_success = send_digest_email(period_label, summary_md)
        
        if email_success:
            logger.info(f"âœ… {period_label}ç”Ÿæˆå¹¶å‘é€æˆåŠŸï¼")
            BOT_STATUS["last_report"] = now_local
            BOT_STATUS["total_reports"] += 1
        else:
            logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥")
            return False
        
        logger.info("=" * 80)
        logger.info(f"âœ¨ {period_label}å®Œæˆ")
        logger.info("=" * 80)
        
        return True
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆ{period_label}å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}", exc_info=True)
        
        BOT_STATUS["errors"].append({
            "time": datetime.now(),
            "error": error_msg
        })
        
        # å‘é€é”™è¯¯é€šçŸ¥é‚®ä»¶
        try:
            send_error_notification(error_msg)
        except:
            pass
        
        return False


def start_scheduler():
    """å¯åŠ¨å®šæ—¶è°ƒåº¦å™¨"""
    if scheduler.running:
        logger.warning("è°ƒåº¦å™¨å·²åœ¨è¿è¡Œä¸­")
        return
    
    # æ¸…é™¤ç°æœ‰ä»»åŠ¡
    scheduler.remove_all_jobs()
    
    # æ·»åŠ å®šæ—¶ä»»åŠ¡
    report_times = CFG.get("report_times", ["09:00", "21:00"])
    
    for report_time in report_times:
        try:
            hour, minute = map(int, report_time.split(":"))
            label = "æ—©æŠ¥" if hour < 12 else "æ™šæŠ¥"
            
            scheduler.add_job(
                generate_and_send_digest,
                CronTrigger(hour=hour, minute=minute, timezone=TZNAME),
                args=[label],
                id=f"daily_{label}_{report_time}",
                name=f"{label}({report_time})",
                replace_existing=True
            )
            
            logger.info(f"âœ… å·²æ·»åŠ å®šæ—¶ä»»åŠ¡: {label} - æ¯å¤© {report_time}")
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å®šæ—¶ä»»åŠ¡å¤±è´¥ ({report_time}): {e}")
    
    # å¯åŠ¨è°ƒåº¦å™¨
    scheduler.start()
    logger.info("ğŸš€ è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    # æ‰“å°ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
    jobs = scheduler.get_jobs()
    for job in jobs:
        next_run = job.next_run_time
        logger.info(f"   ğŸ“… {job.name} ä¸‹æ¬¡æ‰§è¡Œ: {next_run}")


def stop_scheduler():
    """åœæ­¢è°ƒåº¦å™¨"""
    if scheduler.running:
        scheduler.shutdown(wait=False)
        logger.info("â¸ï¸ è°ƒåº¦å™¨å·²åœæ­¢")


def show_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    print("\n" + "=" * 80)
    print("ğŸ“Š BioRxiv è‚¿ç˜¤å­¦ç ”ç©¶æ¨é€ç³»ç»Ÿ - çŠ¶æ€ç›‘æ§")
    print("=" * 80)
    
    uptime = datetime.now() - BOT_STATUS["start_time"]
    uptime_str = str(uptime).split('.')[0]
    
    print(f"\nğŸŸ¢ è¿è¡ŒçŠ¶æ€: {'è¿è¡Œä¸­' if BOT_STATUS['running'] else 'å·²åœæ­¢'}")
    print(f"â±ï¸  è¿è¡Œæ—¶é•¿: {uptime_str}")
    print(f"ğŸ“ˆ å·²ç”ŸæˆæŠ¥å‘Š: {BOT_STATUS['total_reports']} ä»½")
    print(f"ğŸ“„ å·²å¤„ç†æ–‡ç« : {BOT_STATUS['total_papers']} ç¯‡")
    print(f"ğŸŒ æ—¶åŒº: {TZNAME}")
    print(f"â° æŠ¥é€æ—¶é—´: {', '.join(CFG.get('report_times', []))}")
    print(f"âŒ› æ—¶é—´çª—å£: {WINDOW_H} å°æ—¶")
    
    if BOT_STATUS["last_fetch"]:
        print(f"ğŸ“¥ æœ€åè·å–: {BOT_STATUS['last_fetch'].strftime('%Y-%m-%d %H:%M:%S')}")
    
    if BOT_STATUS["last_report"]:
        print(f"ğŸ“§ æœ€åæŠ¥å‘Š: {BOT_STATUS['last_report'].strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ˜¾ç¤ºå®šæ—¶ä»»åŠ¡
    if scheduler.running:
        jobs = scheduler.get_jobs()
        print(f"\nğŸ“… å®šæ—¶ä»»åŠ¡ ({len(jobs)} ä¸ª):")
        for job in jobs:
            next_run = job.next_run_time
            print(f"   â€¢ {job.name}: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ˜¾ç¤ºé”™è¯¯
    if BOT_STATUS["errors"]:
        recent_errors = BOT_STATUS["errors"][-3:]
        print(f"\nâš ï¸  æœ€è¿‘é”™è¯¯ ({len(BOT_STATUS['errors'])} ä¸ª):")
        for err in recent_errors:
            print(f"   â€¢ {err['time'].strftime('%H:%M:%S')}: {err['error']}")
    
    print("\n" + "=" * 80 + "\n")


async def manual_run(period_type="auto"):
    """
    æ‰‹åŠ¨è¿è¡Œä¸€æ¬¡æŠ¥å‘Šç”Ÿæˆ
    
    Args:
        period_type: æŠ¥å‘Šç±»å‹ (am/pm/auto)
    """
    if period_type == "auto":
        # æ ¹æ®å½“å‰æ—¶é—´è‡ªåŠ¨åˆ¤æ–­
        now_local = now_in_tz(TZNAME)
        period_type = "am" if now_local.hour < 12 else "pm"
    
    label = "æ—©æŠ¥" if period_type.lower() == "am" else "æ™šæŠ¥"
    
    logger.info(f"ğŸ¯ æ‰‹åŠ¨è§¦å‘: {label}")
    success = await generate_and_send_digest(label, manual=True)
    
    if success:
        logger.info(f"âœ… æ‰‹åŠ¨ç”Ÿæˆ{label}æˆåŠŸ")
    else:
        logger.error(f"âŒ æ‰‹åŠ¨ç”Ÿæˆ{label}å¤±è´¥")
    
    return success


async def main_loop():
    """ä¸»å¾ªç¯"""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸš€ BioRxiv è‚¿ç˜¤å­¦ç ”ç©¶æ¨é€ç³»ç»Ÿå¯åŠ¨")
    logger.info("=" * 80)
    logger.info(f"ğŸ“… å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ğŸŒ æ—¶åŒº: {TZNAME}")
    logger.info(f"â° æŠ¥é€æ—¶é—´: {', '.join(CFG.get('report_times', []))}")
    logger.info(f"ğŸ“§ æ”¶ä»¶äºº: {os.getenv('EMAIL_RECIPIENT', 'æœªé…ç½®')}")
    logger.info("=" * 80 + "\n")
    
    # å¯åŠ¨è°ƒåº¦å™¨
    start_scheduler()
    
    try:
        # ä¿æŒè¿è¡Œ
        while True:
            await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            
    except KeyboardInterrupt:
        logger.info("\næ”¶åˆ°é€€å‡ºä¿¡å·...")
    finally:
        stop_scheduler()
        logger.info("ğŸ‘‹ ç³»ç»Ÿå·²åœæ­¢")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "status" or command == "smi":
            # æ˜¾ç¤ºçŠ¶æ€
            show_status()
            
        elif command == "run" or command == "rn":
            # æ‰‹åŠ¨è¿è¡Œä¸€æ¬¡
            period = sys.argv[2] if len(sys.argv) > 2 else "auto"
            asyncio.run(manual_run(period))
            
        elif command == "test":
            # æµ‹è¯•æ¨¡å¼ï¼šç«‹å³è¿è¡Œä¸€æ¬¡
            logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šç«‹å³ç”Ÿæˆä¸€æ¬¡æŠ¥å‘Š")
            asyncio.run(manual_run("auto"))
            
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            print("\nå¯ç”¨å‘½ä»¤:")
            print("  python biorxiv_bot.py              - å¯åŠ¨æœåŠ¡")
            print("  python biorxiv_bot.py status       - æ˜¾ç¤ºçŠ¶æ€")
            print("  python biorxiv_bot.py run [am|pm]  - æ‰‹åŠ¨è¿è¡Œä¸€æ¬¡")
            print("  python biorxiv_bot.py test         - æµ‹è¯•æ¨¡å¼")
    else:
        # é»˜è®¤ï¼šå¯åŠ¨æœåŠ¡
        asyncio.run(main_loop())

